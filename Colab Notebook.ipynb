{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Manhwa Translator v2\n",
                "\n",
                "Batch processing pipeline for translating Korean manhwa CBZ files to English.\n",
                "\n",
                "**Pipeline:**\n",
                "1. Upload ZIP containing CBZ files\n",
                "2. Extract each CBZ file\n",
                "3. Batch process images (16 at a time)\n",
                "4. Detect bubbles â†’ Extract text â†’ Translate â†’ Render\n",
                "5. Repack processed images back to CBZ\n",
                "6. Download all translated CBZ files as ZIP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install ultralytics easyocr transformers accelerate opencv-python pillow sentencepiece -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download YOLO Bubble Detector Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!wget https://huggingface.co/ogkalu/comic-speech-bubble-detector-yolov8m/resolve/main/comic-speech-bubble-detector.pt -O bubble_detector.pt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Upload ZIP File (containing CBZ files)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import zipfile\n",
                "import shutil\n",
                "from google.colab import files\n",
                "\n",
                "# Create directories\n",
                "BASE_DIR = 'manhwa_work'\n",
                "INPUT_DIR = os.path.join(BASE_DIR, 'input')\n",
                "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
                "TEMP_DIR = os.path.join(BASE_DIR, 'temp')\n",
                "\n",
                "for d in [INPUT_DIR, OUTPUT_DIR, TEMP_DIR]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "# Upload ZIP file\n",
                "print(\"Please upload your ZIP file containing CBZ files:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Extract ZIP\n",
                "zip_filename = list(uploaded.keys())[0]\n",
                "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
                "    zip_ref.extractall(INPUT_DIR)\n",
                "\n",
                "# Find all CBZ files\n",
                "cbz_files = []\n",
                "for root, dirs, files_list in os.walk(INPUT_DIR):\n",
                "    for f in files_list:\n",
                "        if f.lower().endswith('.cbz'):\n",
                "            cbz_files.append(os.path.join(root, f))\n",
                "\n",
                "cbz_files.sort()\n",
                "print(f\"\\nâœ… Found {len(cbz_files)} CBZ files:\")\n",
                "for cbz in cbz_files:\n",
                "    print(f\"  - {os.path.basename(cbz)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from ultralytics import YOLO\n",
                "import easyocr\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "import cv2\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Load YOLO bubble detector\n",
                "print(\"Loading YOLO bubble detector...\")\n",
                "bubble_model = YOLO('bubble_detector.pt')\n",
                "\n",
                "# Load EasyOCR reader for Korean\n",
                "print(\"Loading EasyOCR (Korean)...\")\n",
                "ocr_reader = easyocr.Reader(['ko'], gpu=torch.cuda.is_available())\n",
                "\n",
                "# Load NLLB translation model (fine-tuned for Korean to English)\n",
                "print(\"Loading NLLB fine-tuned Ko2En translation model...\")\n",
                "nllb_model_name = \"NHNDQ/nllb-finetuned-ko2en\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(nllb_model_name)\n",
                "translation_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_name).to(device)\n",
                "\n",
                "print(\"Using OpenCV inpainting...\")\n",
                "print(\"\\nâœ… All models loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Define Processing Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "import textwrap\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "\n",
                "# Font path for Colab\n",
                "FONT_PATH = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
                "try:\n",
                "    ImageFont.truetype(FONT_PATH, 12)\n",
                "except:\n",
                "    FONT_PATH = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
                "\n",
                "def translate_text(korean_text):\n",
                "    \"\"\"Translate Korean text to English using fine-tuned NLLB.\"\"\"\n",
                "    if not korean_text:\n",
                "        return \"\"\n",
                "    \n",
                "    inputs = tokenizer(korean_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
                "    \n",
                "    translated_tokens = translation_model.generate(\n",
                "        **inputs,\n",
                "        max_length=256,\n",
                "        num_beams=5,\n",
                "        early_stopping=True\n",
                "    )\n",
                "    \n",
                "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
                "    return translated_text\n",
                "\n",
                "def translate_batch(texts):\n",
                "    \"\"\"Translate a batch of Korean texts to English.\"\"\"\n",
                "    if not texts:\n",
                "        return []\n",
                "    \n",
                "    # Filter empty texts\n",
                "    valid_indices = [i for i, t in enumerate(texts) if t.strip()]\n",
                "    valid_texts = [texts[i] for i in valid_indices]\n",
                "    \n",
                "    if not valid_texts:\n",
                "        return [\"\"] * len(texts)\n",
                "    \n",
                "    inputs = tokenizer(valid_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
                "    \n",
                "    translated_tokens = translation_model.generate(\n",
                "        **inputs,\n",
                "        max_length=256,\n",
                "        num_beams=5,\n",
                "        early_stopping=True\n",
                "    )\n",
                "    \n",
                "    translations = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
                "    \n",
                "    # Map back to original indices\n",
                "    result = [\"\"] * len(texts)\n",
                "    for i, idx in enumerate(valid_indices):\n",
                "        result[idx] = translations[i]\n",
                "    \n",
                "    return result\n",
                "\n",
                "def process_single_image(image_path):\n",
                "    \"\"\"Process a single image: detect bubbles, extract text, translate, white-out and render.\"\"\"\n",
                "    img = cv2.imread(image_path)\n",
                "    if img is None:\n",
                "        return None, []\n",
                "    \n",
                "    # Detect bubbles\n",
                "    yolo_result = bubble_model(image_path, conf=0.5, verbose=False)\n",
                "    \n",
                "    # Convert to PIL for drawing\n",
                "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
                "    draw = ImageDraw.Draw(pil_img)\n",
                "    \n",
                "    bubble_data = []\n",
                "    \n",
                "    # Process each detected bubble - extract text\n",
                "    for bubble in yolo_result[0].boxes:\n",
                "        bx1, by1, bx2, by2 = map(int, bubble.xyxy[0])\n",
                "        \n",
                "        bubble_crop = img[by1:by2, bx1:bx2]\n",
                "        ocr_res = ocr_reader.readtext(bubble_crop)\n",
                "        \n",
                "        if not ocr_res:\n",
                "            continue\n",
                "        \n",
                "        korean_text = ' '.join([res[1] for res in ocr_res])\n",
                "        \n",
                "        # Store bubble info for batch translation\n",
                "        bubble_data.append({\n",
                "            'box': (bx1, by1, bx2, by2),\n",
                "            'korean_text': korean_text,\n",
                "            'ocr_res': ocr_res\n",
                "        })\n",
                "    \n",
                "    return (img, pil_img, draw, bubble_data), [b['korean_text'] for b in bubble_data]\n",
                "\n",
                "def render_translated_image(img_data, translations):\n",
                "    \"\"\"Render translated text onto image.\"\"\"\n",
                "    if img_data is None:\n",
                "        return None\n",
                "    \n",
                "    img, pil_img, draw, bubble_data = img_data\n",
                "    \n",
                "    for i, data in enumerate(bubble_data):\n",
                "        bx1, by1, bx2, by2 = data['box']\n",
                "        english_text = translations[i] if i < len(translations) else \"\"\n",
                "        \n",
                "        # WHITE-OUT: Draw white rectangles over Korean text\n",
                "        for res in data['ocr_res']:\n",
                "            poly = np.array(res[0], dtype=np.int32)\n",
                "            x_min = poly[:, 0].min() + bx1\n",
                "            y_min = poly[:, 1].min() + by1\n",
                "            x_max = poly[:, 0].max() + bx1\n",
                "            y_max = poly[:, 1].max() + by1\n",
                "            draw.rectangle([x_min-2, y_min-2, x_max+2, y_max+2], fill=\"white\")\n",
                "        \n",
                "        if not english_text:\n",
                "            continue\n",
                "        \n",
                "        # Render English text\n",
                "        padding = 10\n",
                "        target_w = (bx2 - bx1) - (2 * padding)\n",
                "        target_h = (by2 - by1) - (2 * padding)\n",
                "        \n",
                "        font_size = int(img.shape[0] / 50)\n",
                "        font = ImageFont.truetype(FONT_PATH, font_size)\n",
                "        \n",
                "        while font_size > 8:\n",
                "            font = ImageFont.truetype(FONT_PATH, font_size)\n",
                "            avg_char_w = font.getlength(\"a\")\n",
                "            chars_per_line = max(1, int(target_w / avg_char_w))\n",
                "            wrapped = textwrap.wrap(english_text, width=chars_per_line, break_long_words=False)\n",
                "            total_text_h = len(wrapped) * (font_size + 4)\n",
                "            if total_text_h <= target_h:\n",
                "                break\n",
                "            font_size -= 2\n",
                "        \n",
                "        current_y = by1 + (target_h - total_text_h) // 2 + padding\n",
                "        for line in wrapped:\n",
                "            bbox_line = draw.textbbox((0, 0), line, font=font)\n",
                "            line_w = bbox_line[2] - bbox_line[0]\n",
                "            current_x = bx1 + (target_w - line_w) // 2 + padding\n",
                "            draw.text((current_x, current_y), line, font=font, fill=\"black\")\n",
                "            current_y += font_size + 4\n",
                "    \n",
                "    return pil_img\n",
                "\n",
                "def process_batch(image_paths, output_dir):\n",
                "    \"\"\"Process a batch of images.\"\"\"\n",
                "    # Step 1: Process all images (YOLO + OCR)\n",
                "    all_img_data = []\n",
                "    all_texts = []\n",
                "    text_counts = []\n",
                "    \n",
                "    for img_path in image_paths:\n",
                "        img_data, texts = process_single_image(img_path)\n",
                "        all_img_data.append((img_path, img_data))\n",
                "        all_texts.extend(texts)\n",
                "        text_counts.append(len(texts))\n",
                "    \n",
                "    # Step 2: Batch translate all texts\n",
                "    if all_texts:\n",
                "        all_translations = translate_batch(all_texts)\n",
                "    else:\n",
                "        all_translations = []\n",
                "    \n",
                "    # Step 3: Render and save\n",
                "    translation_idx = 0\n",
                "    for i, (img_path, img_data) in enumerate(all_img_data):\n",
                "        count = text_counts[i]\n",
                "        translations = all_translations[translation_idx:translation_idx + count]\n",
                "        translation_idx += count\n",
                "        \n",
                "        if img_data is None or img_data[0] is None:\n",
                "            # Copy original if processing failed\n",
                "            src = img_path\n",
                "            dst = os.path.join(output_dir, os.path.basename(img_path))\n",
                "            shutil.copy2(src, dst)\n",
                "            continue\n",
                "        \n",
                "        result_img = render_translated_image(img_data, translations)\n",
                "        if result_img:\n",
                "            output_path = os.path.join(output_dir, os.path.basename(img_path))\n",
                "            result_img.save(output_path)\n",
                "        else:\n",
                "            shutil.copy2(img_path, os.path.join(output_dir, os.path.basename(img_path)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Process All CBZ Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm import tqdm\n",
                "import math\n",
                "\n",
                "BATCH_SIZE = 16\n",
                "\n",
                "print(f\"Processing {len(cbz_files)} CBZ files...\\n\")\n",
                "\n",
                "for cbz_path in cbz_files:\n",
                "    cbz_name = os.path.basename(cbz_path)\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"ðŸ“š Processing: {cbz_name}\")\n",
                "    \n",
                "    # Create temp directories for this CBZ\n",
                "    cbz_temp_input = os.path.join(TEMP_DIR, 'cbz_input')\n",
                "    cbz_temp_output = os.path.join(TEMP_DIR, 'cbz_output')\n",
                "    \n",
                "    # Clean and create temp dirs\n",
                "    for d in [cbz_temp_input, cbz_temp_output]:\n",
                "        if os.path.exists(d):\n",
                "            shutil.rmtree(d)\n",
                "        os.makedirs(d)\n",
                "    \n",
                "    # Extract CBZ (it's just a ZIP file)\n",
                "    try:\n",
                "        with zipfile.ZipFile(cbz_path, 'r') as zip_ref:\n",
                "            zip_ref.extractall(cbz_temp_input)\n",
                "    except Exception as e:\n",
                "        print(f\"  âš ï¸ Error extracting CBZ: {e}\")\n",
                "        continue\n",
                "    \n",
                "    # Get all files from extracted CBZ\n",
                "    image_extensions = ('.png', '.jpg', '.jpeg', '.webp', '.gif')\n",
                "    image_files = []\n",
                "    non_image_files = []\n",
                "    \n",
                "    for root, dirs, files_list in os.walk(cbz_temp_input):\n",
                "        for f in files_list:\n",
                "            if f.startswith('.'):\n",
                "                continue\n",
                "            full_path = os.path.join(root, f)\n",
                "            rel_path = os.path.relpath(full_path, cbz_temp_input)\n",
                "            \n",
                "            if f.lower().endswith(image_extensions):\n",
                "                image_files.append(full_path)\n",
                "            else:\n",
                "                # Copy non-image files directly to output (preserve metadata like ComicInfo.xml)\n",
                "                non_image_files.append((full_path, rel_path))\n",
                "    \n",
                "    image_files.sort()\n",
                "    print(f\"  Found {len(image_files)} images, {len(non_image_files)} other files\")\n",
                "    \n",
                "    # Copy non-image files to output directory (preserving folder structure)\n",
                "    for src_path, rel_path in non_image_files:\n",
                "        dst_path = os.path.join(cbz_temp_output, rel_path)\n",
                "        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
                "        shutil.copy2(src_path, dst_path)\n",
                "    \n",
                "    if not image_files:\n",
                "        print(f\"  âš ï¸ No images found, skipping...\")\n",
                "        continue\n",
                "    \n",
                "    # Process in batches\n",
                "    num_batches = math.ceil(len(image_files) / BATCH_SIZE)\n",
                "    \n",
                "    for batch_idx in tqdm(range(num_batches), desc=f\"  Batches\"):\n",
                "        start_idx = batch_idx * BATCH_SIZE\n",
                "        end_idx = min(start_idx + BATCH_SIZE, len(image_files))\n",
                "        batch_images = image_files[start_idx:end_idx]\n",
                "        \n",
                "        process_batch(batch_images, cbz_temp_output)\n",
                "    \n",
                "    # Repack to CBZ\n",
                "    output_cbz_path = os.path.join(OUTPUT_DIR, cbz_name)\n",
                "    \n",
                "    with zipfile.ZipFile(output_cbz_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
                "        for root, dirs, files_list in os.walk(cbz_temp_output):\n",
                "            for f in sorted(files_list):\n",
                "                file_path = os.path.join(root, f)\n",
                "                arcname = os.path.relpath(file_path, cbz_temp_output)\n",
                "                zipf.write(file_path, arcname)\n",
                "    \n",
                "    print(f\"  âœ… Saved: {cbz_name}\")\n",
                "\n",
                "# Clean up temp directory\n",
                "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"âœ… All CBZ files processed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Create & Download Output ZIP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Create output ZIP\n",
                "output_zip = 'translated_manhwa.zip'\n",
                "\n",
                "with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
                "    for root, dirs, files_list in os.walk(OUTPUT_DIR):\n",
                "        for f in files_list:\n",
                "            if f.lower().endswith('.cbz'):\n",
                "                file_path = os.path.join(root, f)\n",
                "                arcname = os.path.relpath(file_path, OUTPUT_DIR)\n",
                "                zipf.write(file_path, arcname)\n",
                "\n",
                "# Count CBZ files in output\n",
                "output_cbz_count = len([f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith('.cbz')])\n",
                "print(f\"âœ… Created {output_zip} with {output_cbz_count} translated CBZ files\")\n",
                "\n",
                "# Download\n",
                "print(\"Starting download...\")\n",
                "files.download(output_zip)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Cleanup Storage (Optional)\n",
                "\n",
                "Run this cell to remove all uploaded and processed files to free up Colab storage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import shutil\n",
                "# import os\n",
                "\n",
                "# # Remove work directories\n",
                "# if os.path.exists(BASE_DIR):\n",
                "#     shutil.rmtree(BASE_DIR)\n",
                "#     print(f\"âœ… Removed {BASE_DIR}/\")\n",
                "\n",
                "# # Remove uploaded ZIP\n",
                "# if 'zip_filename' in dir() and os.path.exists(zip_filename):\n",
                "#     os.remove(zip_filename)\n",
                "#     print(f\"âœ… Removed {zip_filename}\")\n",
                "\n",
                "# # Remove output ZIP\n",
                "# if os.path.exists('translated_manhwa.zip'):\n",
                "#     os.remove('translated_manhwa.zip')\n",
                "#     print(\"âœ… Removed translated_manhwa.zip\")\n",
                "\n",
                "# # Remove YOLO model\n",
                "# if os.path.exists('bubble_detector.pt'):\n",
                "#     os.remove('bubble_detector.pt')\n",
                "#     print(\"âœ… Removed bubble_detector.pt\")\n",
                "\n",
                "# print(\"\\nðŸ§¹ Cleanup complete!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
